[global]
# attached 1T SSD data disk; change this directory name as appropriate
directory=/mnt/disks/data
# use pthread_create instead of fork
thread=1
# avoid caching
direct=1
# avoid avoiding randomization
randrepeat=0
# don't muddy the results with a verification step
do_verify=0
# name of pre-created test file to use
filename=populate
# set to a little less than the size of the disk - run populate section first
size=900G
# using a time-based job
time_based=1
# run each test for this time
runtime=900s
# let performance settle for 5 seconds before logging stats
ramp_time=5s
# combine statistics for multiple jobs per test
group_reporting=1
# avoid overwriting same block in async i/o
random_generator=lfsr

# Note: the logging is prolific in these tests, but
# you should really be looking at bandwidth logs for bandwidth tests,
# latency logs for latency tests, 
# and iops logs for iops tests -
# full logging is included if you want to see, for example,
# whether bandwidth is unintentionally saturated during a latency test

[write-latency]
wait_for_previous
# avoid parallelism to get pure latency
ioengine=psync
readwrite=randwrite
# size of i/o chunks; default
blocksize=4k
# note: iodepth >1 is meaningless with psync
# note: intentionally running this test with one thread (job)
# log individual data points
write_lat_log=4k-write-lat-latency.results
write_iops_log=4k-write-late-iops.results
write_bw_log=4k-write-lat-bw.results

[read-latency]
wait_for_previous
# avoid parallelism to get pure latency
ioengine=psync
readwrite=randread
# size of i/o chunks; default
blocksize=4k
# note: iodepth >1 is meaningless with psync
# note: intentionally running this test with one thread (job)
# log individual data points
write_lat_log=4k-read-lat-latency.results
write_iops_log=4k-read-lat-iops.results
write_bw_log=4k-read-lat-bw.results

[write-bandwidth]
wait_for_previous
ioengine=libaio
readwrite=write
# size of i/o chunks; <=256K if testing Azure, b/c every 256K is counted as 1 i/o
blocksize=256k
# 8 concurrent threads will run for bandwidth tests
numjobs=8
# assumes 16 cores
iodepth=1024
# log individual data points
write_lat_log=256k-write-bw-latency.results
write_iops_log=256k-write-bw-iops.results
write_bw_log=256k-write-bw-bw.results

[read-bandwidth]
wait_for_previous
ioengine=libaio
readwrite=randread
# size of i/o chunks; <=256K if testing Azure, b/c every 256K is counted as 1 i/o
blocksize=256k
# 8 concurrent threads will run for bandwidth tests
numjobs=8
# assumes 16 cores
iodepth=1024
# log individual data points
write_lat_log=256k-read-bw-latency.results
write_iops_log=256k-read-bw-iops.results
write_bw_log=256k-read-bw-bw.results

[write-iops]
wait_for_previous
ioengine=libaio
readwrite=randwrite
# size of i/o chunks; default
blocksize=4k
# 8 concurrent threads will run for iops tests
numjobs=8
# assumes 16 cores
iodepth=8192
# log individual data points
write_lat_log=4k-write-iops-latency.results
write_iops_log=4k-write-ops-iops.results
write_bw_log=4k-write-iops-bw.results

[read-iops]
wait_for_previous
ioengine=libaio
readwrite=randread
# size of i/o chunks; default
blocksize=4k
# 8 concurrent threads will run for iops tests
numjobs=8
# assumes 16 cores
iodepth=8192
# log individual data points
write_lat_log=4k-read-iops-latency.results
write_iops_log=4k-read-iops-iops.results
write_bw_log=4k-read-iops-bw.results
